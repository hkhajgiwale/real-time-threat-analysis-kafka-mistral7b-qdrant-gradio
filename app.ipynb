{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b74a136-bc3b-4a8e-961b-cd5de9f4fa3f",
   "metadata": {},
   "source": [
    "## Real time Threat Analysis Application using Apache Kafka, Qdrant VectorDB, Mistral 7B, Langchain and Gradio\n",
    "\n",
    "##### Consuming message from Kafka\n",
    "\n",
    "##### For now we are stopping after consuming 1 million messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3af26-f307-47dd-ba78-c4c8757349b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "import json\n",
    "import time\n",
    "\n",
    "### Function to fetch the data from the consumer\n",
    "def process_kafka_message(consumer):\n",
    "    message = consumer.poll(timeout=0.5)\n",
    "    if message is None:\n",
    "        return None\n",
    "    if message.error():\n",
    "        if message.error().code() == KafkaError._PARTITION_EOF:\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Error: {message.error()}\")\n",
    "            return None\n",
    "    try:\n",
    "        event_data = json.loads(message.value().decode('utf-8'))\n",
    "        return event_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing message: {e}\")\n",
    "        return None\n",
    "\n",
    "## Consumer config\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'threat_analytics_consumer_group',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'enable.auto.commit': False\n",
    "}\n",
    "consumer = Consumer(consumer_config)\n",
    "consumer.subscribe(['threat-analytics-topic'])\n",
    "\n",
    "\n",
    "event_data_list = []\n",
    "count = 0  # Count the number of messages consumed\n",
    "\n",
    "### For now we are consuming a million messages and storing it in the array\n",
    "try:\n",
    "    while True:\n",
    "        event_data = process_kafka_message(consumer)\n",
    "        if event_data is not None:\n",
    "            event_data_list.append(event_data)\n",
    "            count += 1\n",
    "        if count >= 1000000:\n",
    "            break  # Exit the loop after 50,000 messages\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb732df7-466f-48da-a473-d8c2467df111",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db954f2b-e83d-4dd0-949d-a13b4d556d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "def create_docs(event_data_list):\n",
    "    json_splitter = RecursiveJsonSplitter(max_chunk_size=1000)\n",
    "    docs = json_splitter.create_documents(texts=event_data_list)\n",
    "    return docs\n",
    "\n",
    "docs = create_docs(event_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a2687-e5d2-4d75-84ba-121cbc0753a6",
   "metadata": {},
   "source": [
    "## Connecting with Qdrant VectorDB and creating the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ff2d3-4e2e-431e-a0b4-e64e621c4232",
   "metadata": {},
   "source": [
    "##### Since there is huge amount of data we are first initialising with 1000 elements\n",
    "##### Later will perform the batch processing with 10000 elements in one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf0ee0-24ff-499e-8a43-7c04415204e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "url=\"http://localhost:6333\"\n",
    "\n",
    "doc_store = Qdrant.from_documents(docs[:1000], \n",
    "                                  HuggingFaceEmbeddings(), \n",
    "                                  url=url,\n",
    "                                  grpc_port=6334,\n",
    "                                  force_recreate=True,\n",
    "                                  prefer_grpc=True,\n",
    "                                  collection_name=\"threat_analytics_vector\",\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9c017-6dce-43f6-8ea9-a645fc4736a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1000  # Define your preferred batch size\n",
    "for i in range(1000, len(docs), batch_size):\n",
    "        chunk_batch = docs[i:i + batch_size]\n",
    "        doc_store.add_documents(chunk_batch)\n",
    "print(\"Documents added to Vector Store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804f44b-4382-4d45-b916-e8655786eb4d",
   "metadata": {},
   "source": [
    "## Creating the LLM model using Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f6608-da5a-4ed0-8c2d-ad48b86037df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "\n",
    "def load_llm():\n",
    "\n",
    "    #Loading the Mistral Model\n",
    "    model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        )\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=True,\n",
    "        max_new_tokens=1024,\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb06ec-55b6-4a1f-bb0d-8f334c2e95f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = load_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed9ed0-6ac5-4c12-98f7-dba5c2e72226",
   "metadata": {},
   "source": [
    "## Function to answer the query from the User\n",
    "#### 1. Writing the prompt\n",
    "#### 2. Creating the rag_chain with StrOutputParser and RunnablePassThrough\n",
    "#### 3. Feeding the rendered template to the LLM and parsing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f1e84-e582-46d5-ac03-9ec1456dffb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def answer_query(question, llm, doc_store):\n",
    "    context_docs = doc_store.similarity_search(question, k= 4)\n",
    "    context = ' '.join(doc.page_content for doc in context_docs)\n",
    "\n",
    "    template = f\"\"\"You are smart bot. You primarily possess rich expertise in analysing cybersecurity threats.\n",
    "Below provided is the content of json that you would get from the store\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "\n",
    "Use the following information to answer the user's question. These are the system related information of particular system belonging to the user.\n",
    "\n",
    "default fields:\n",
    "\n",
    "username: username of the system\n",
    "ip_address: ip address of the system\n",
    "user_agent: User Agent is typically the web browser of the system\n",
    "attack_types: different types of attacks that can happen over the system. Understand that system has been the victim of this attack\n",
    "threat_actors: Threat actors are individuals, groups, or organizations that pose a threat to computer systems, networks, or data and can include a wide range of entities, such as hackers, cybercriminals, hacktivists, state-sponsored groups, and insiders\n",
    "cwe: CWE stands for Common Weakness Enumeration. It is a community-developed list of software and hardware weakness types that can serve as a common language for describing software security vulnerabilities. \n",
    "cve: CVE stands for Common Vulnerabilities and Exposures and Each CVE ID is associated with a description of the vulnerability, including details such as affected products, versions, and potential impact.\n",
    "affected_resource: Its the file that gets affected negatively usually because of the attack\n",
    "timestamp: Time at which the attack happened\n",
    "\n",
    "If somebody asks if how many systems were impacted by a particular attack or any other particular field, go through the doc store entirely and calculate the sum of the systems with that particular field.\n",
    "\n",
    "A few examples-\n",
    "\n",
    "Q: What happened with daniel00's system having ip address 113.175.192.202\n",
    "A: Sure here's what happened - daniel's system was victim of man-in-the-middle attack and corrupted /own/assume.wav. The threat actor is Script Kiddie with CWE is CWE-229 and CVE to be CVE-2023-3757.\n",
    "\n",
    "In case you don't know the answer, just say that you don't know, don't try to make up an answer. Only return the helpful answer below and nothing else.\n",
    "\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = (\n",
    "        {\"context\": doc_store.as_retriever(search_kwargs={'k': 10}), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = llm(template)\n",
    "\n",
    "    answer = result.replace(template, '')\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926da34-ec9b-469e-8d54-73e5576ce916",
   "metadata": {},
   "source": [
    "## Initialising the Gradio and invoking answer_query function for generating response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a0b35-0247-4b19-8e93-b421b24aa715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "def slow_echo(question, history):\n",
    "    response = answer_query(question, llm, doc_store)\n",
    "    for i in range(len(response)):\n",
    "        time.sleep(0.1)\n",
    "        #yield \"You typed: \" + message[: i+1]\n",
    "        yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(slow_echo).launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf2feb-7c34-46af-9704-e72811d996cb",
   "metadata": {},
   "source": [
    "![SNOWFALL](gradio_output.png) \n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
